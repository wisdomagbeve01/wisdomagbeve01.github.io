---
layout: post
title: "Day 20 – Kicking Off Model Training"
date: 2025-06-14
author: Wisdom Agbeve
permalink: /day20.html
tags: ["Python", "Machine Learning", "Model Training", "XGBoost", "LSTM"]
  
what_i_learned: |
  Today marked the beginning of our **model training phase**! After all the effort spent on cleaning and preparing our data last week, it felt exciting to finally start building models.

  We began by reviewing some foundational machine learning concepts and discussed different algorithms that could work for our dataset. As a team, we decided to start with **XGBoost** for its speed and accuracy on tabular data. Some of us also began setting up for **LSTM** since we’re working with time-series features.

  I learned how to:
  - Split our dataset into training and testing sets using `train_test_split()`
  - Initialize and train an XGBoost model in Python
  - Evaluate our model using metrics like **RMSE**, **MAE**, and **R²**
  - Save early results and performance scores for tracking progress

  We also looked at the importance of avoiding overfitting, and how to tune hyperparameters like learning rate, number of estimators, and max depth.

blockers: |
  Some model outputs didn't perform as expected at first — especially when we forgot to normalize certain features. It took a few tries to get the preprocessing pipeline right before model training.

reflection: |
  It felt great to finally move from preparation to prediction. Today showed me how important all the earlier steps were — from feature selection to data formatting. Machine learning isn’t just about coding a model; it’s about setting up the right conditions for it to succeed.

  I’m looking forward to trying out LSTM next and comparing how it performs against XGBoost. We're one step closer to having a working prototype!
---
