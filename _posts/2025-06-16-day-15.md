---
layout: post
title: "Day 15 – Data Cleaning, SQL & Time Series Kickoff"
date: 2025-06-16
author: Wisdom Agbeve
permalink: /day15.html
tags: ["Python", "Pandas", "EDA", "Time Series", "SQL", "Data Preprocessing"]

what_i_learned: |
  Today was a deep dive into preparing our datasets for the core modeling phase of our flight delay prediction project. I began by cleaning and preprocessing our finalized datasets. This included handling missing values using `.dropna()` and `.fillna()`, standardizing column names for consistency, and checking for duplicates. I also stripped unnecessary whitespace and converted data types where needed, which helped reduce potential errors during merging and analysis.

  After preprocessing, I successfully merged the datasets using `pd.merge()` and started performing **basic EDA (Exploratory Data Analysis)**. I calculated central tendency measures like **mean**, **median**, and **mode**, and used `.describe()` to get a general summary of the data. I visualized trends using **Matplotlib** to plot distributions and line charts, which helped me begin spotting patterns, such as delays during specific times or weather conditions.

  In addition to Python work, I started learning **SQL**, which is important for querying large structured datasets. I practiced commands like `SELECT`, `WHERE`, `GROUP BY`, and `JOIN` to understand how I can later use SQL to manipulate and pull insights from data efficiently.

  Our graduate mentor also shared a resource on **Time Series Analysis**, and I began reviewing its basics—especially how it relates to tracking trends over time, which is key for our project. I’m beginning to think about how to integrate time-based features like day of the week, hour of departure, and seasonal weather into our models.

blockers: |
  While merging datasets, I encountered issues due to inconsistent date formats and different naming conventions across tables. Fixing these required converting strings to datetime objects and renaming columns. It was time-consuming but helped me better understand the importance of thorough data cleaning before analysis.

reflection: |
  Today was one of the most hands-on and technical days so far. Working with real-world messy data gave me a better appreciation of the "data wrangling" stage that every data scientist talks about. It’s not just about building models—it’s about making sure your inputs are clean, reliable, and meaningful. I’m excited to dig deeper into Time Series concepts tomorrow and start thinking more strategically about feature engineering for delay prediction.
---
